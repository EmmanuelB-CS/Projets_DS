{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE CAP GEMINI INVENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectifs de la partie 3**\n",
    "\n",
    "L'objectif ici est simple: cr√©er une pipeline de mod√®les de r√©gression √† entra√Æner puis tester leurs performances sur nos trois datasets que l'on nommera simplement rough_ml, by_hand et by_reg qui on le rappelle ont √©t√© obtenus sans s√©lection des variables, par s√©lection √† la main et par elastic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "rough_ml = pd.read_csv('df_ml/df_for_ml.csv')\n",
    "by_hand = pd.read_csv('df_ml/ml_selected_byhand.csv')\n",
    "by_reg = pd.read_csv('df_ml/ml_selected_byreg.csv')\n",
    "unnorm_target = pd.read_csv('df_ml/unnormalized_target.csv')\n",
    "\n",
    "df_dict = {'rough_ml':rough_ml, 'by_hand':by_hand, 'by_reg':by_reg}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description des mod√®les:\n",
    "---\n",
    "\n",
    "### Mod√®les de R√©gression Implement√©s üìà\n",
    "\n",
    "#### R√©gression Lin√©aire Multiple üßÆ\n",
    "- **Description**: La r√©gression lin√©aire multiple est un mod√®le statistique qui cherche √† pr√©dire une variable d√©pendante √† partir de plusieurs variables ind√©pendantes, en supposant une relation lin√©aire entre elles.\n",
    "- **Utilisation**: Elle est id√©ale pour mettre en lumi√®re des relations simples et pour servir de benchmark pour des \n",
    "mod√®les plus sophistiqu√©s\n",
    "\n",
    "#### Elastic Net üï∏Ô∏è\n",
    "- **Description**: Elastic Net est une m√©thode de r√©gression r√©gularis√©e qui combine les p√©nalit√©s L1 et L2 des r√©gularisations Lasso et Ridge. \n",
    "- **Utilisation**: Tr√®s utile lorsque plusieurs caract√©ristiques sont corr√©l√©es entre elles. Elastic Net peut aider √† r√©duire le surapprentissage en introduisant ces p√©nalit√©s.\n",
    "- **Avantages**: Permet une s√©lection de caract√©ristiques automatique et la r√©gularisation. Performant lorsque le nombre de pr√©dicteurs est tr√®s grand.\n",
    "- **Inconv√©nients**: Difficile de choisir ses param√®tres.\n",
    "\n",
    "#### Random Forest üå≥\n",
    "- **Description**: Un mod√®le qui op√®re en construisant un grand nombre d'arbres de d√©cision au moment de l'entra√Ænement et en produisant la moyenne des pr√©dictions de ces arbres pour la pr√©diction finale.\n",
    "- **Utilisation**: Excellent pour capturer des relations non lin√©aires sans n√©cessiter une transformation manuelle des caract√©ristiques.\n",
    "- **Avantages**: R√©duit le risque de surapprentissage. Importances des caract√©ristiques facilement extractibles ce qui am√©liore l'\"explicabilit√©\" du mod√®le.\n",
    "- **Inconv√©nients**: Moins interpr√©table qu'un mod√®le de r√©gression lin√©aire.\n",
    "\n",
    "#### XGBoost üöÄ\n",
    "- **Description**: XGBoost (Extreme Gradient Boosting) est un algorithme d'apprentissage ensembliste qui construit de mani√®re it√©rative des arbres de d√©cision de mani√®re √† minimiser une fonction de perte.\n",
    "- **Utilisation**: Id√©al pour des performances √©lev√©es et la rapidit√© sur des t√¢ches de pr√©diction.\n",
    "- **Avantages**: Tr√®s performant, capable de g√©rer des donn√©es de grande dimension et des relations non lin√©aires.\n",
    "- **Inconv√©nients**: Peut √™tre sujet au surapprentissage si les hyperparam√®tres ne sont pas bien choisis. Moins intuitif √† comprendre et interpr√©ter.\n",
    "\n",
    "#### CatBoost üê±üöÄ\n",
    "- **Description**: CatBoost est un algorithme de boosting qui utilise des arbres de d√©cision et est optimis√© pour traiter efficacement les variables cat√©gorielles comme l'√©tat d'un bien immobilier etc...\n",
    "- **Utilisation**: Particuli√®rement efficace pour les ensembles de donn√©es avec de nombreuses caract√©ristiques cat√©gorielles.\n",
    "- **Avantages**: Offre une bonne performance avec peu de param√©trage. G√®re bien les donn√©es cat√©gorielles sans pr√©traitement.\n",
    "- **Inconv√©nients**: Encore et toujours un risque non n√©gligeable de surapprentissage, comme pour tous les algorithmes de boosting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211288.86529576036, 133993.18394892054)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mean = unnorm_target['valeur_fonciere'].mean(axis=0) \n",
    "y_std = unnorm_target['valeur_fonciere'].std(axis=0)\n",
    "y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_params = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'Elastic Net': {\n",
    "        'model': ElasticNet(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1, 10],\n",
    "            'l1_ratio': [0.1, 0.5, 0.9]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'max_depth': [None, 2, 3, 4]\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(),\n",
    "        'params': {\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.5],\n",
    "            'max_depth': [None, 2, 3, 4],\n",
    "            'objective': ['reg:squarederror']\n",
    "        }\n",
    "    },\n",
    "    #'CatBoost': {\n",
    "    #    'model': CatBoostRegressor(verbose=0),  # verbose=0 emp√™che les spams\n",
    "    #    'params': {\n",
    "    #        'iterations': [10, 50, 100],\n",
    "    #        'learning_rate': [0.01, 0.1, 0.5],\n",
    "    #        'depth': [2, 3, 4],\n",
    "    #        'loss_function': ['RMSE']\n",
    "    #    }\n",
    "    #}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Note importante:**</span> \n",
    "\n",
    "Ici nous donnons comme m√©triques d'√©valuation la MSE, le R2 score ainsi que le MAPE. Ce dernier peut sembler peu pertinent du fait que les √©tiquettes soient normalis√©es (et MAPE est TRES sensible au petites valeurs) ou que xgboost ne l'utilise pas dans notre exemple comme fonction de perte... Cependant nous allons tout de m√™me calculer une version modifi√©e par nos soins de cette m√©trique pour satisfaire un double objectif: mettre en perspective la performance du mod√®le avec de nouvelles m√©triques, retourner une m√©trique interpr√©table pour le client.\n",
    "\n",
    "Expliquons en quoi nous l'avons modifi√©e:\n",
    "- La formule initiale \n",
    "\n",
    "`mape = np.mean(np.abs((y_pred - y_test) / y_test)) * 100`\n",
    "\n",
    "- Sa version modifi√©e \n",
    "\n",
    "`modified_mape = np.mean(np.abs((y_test - y_pred) * y_std / (y_test*y_std + y_mean))) * 100`\n",
    "\n",
    "Cette deuxi√®me version multiplie l'√©cart de pr√©diction par la std r√©elle de y afin de rendre compte de ce que cet √©cart observ√© signifie r√©ellement lorsque l'on consid√®re les valeurs foncieres non normalis√©e (std est consid√©r√©e ici comme un √©cart moyen de valeurs √† leur moyenne) \n",
    "\n",
    "On divise par y_test*y_std + y_mean afin d'effectuer un rapport sur une variable 'd√©normalis√©e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- stats for :  rough_ml --------------------------------------------------\n",
      "------------------------------ Linear Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 273826880774065061888.00\n",
      "Linear Regression R2: -262964839779657318400.00\n",
      "Linear Regression MAPE: 311597388433.63\n",
      "------------------------------ Elastic Net\n",
      "Elastic Net MSE: 0.20\n",
      "Elastic Net R2: 0.81\n",
      "Elastic Net MAPE: 21.26\n",
      "------------------------------ Random Forest\n",
      "Random Forest MSE: 0.15\n",
      "Random Forest R2: 0.85\n",
      "Random Forest MAPE: 16.82\n",
      "------------------------------ XGBoost\n",
      "XGBoost MSE: 0.16\n",
      "XGBoost R2: 0.84\n",
      "XGBoost MAPE: 18.06\n",
      "-------------------------------------------------- stats for :  by_hand --------------------------------------------------\n",
      "------------------------------ Linear Regression\n",
      "Linear Regression MSE: 0.18\n",
      "Linear Regression R2: 0.83\n",
      "Linear Regression MAPE: 21.64\n",
      "------------------------------ Elastic Net\n",
      "Elastic Net MSE: 0.20\n",
      "Elastic Net R2: 0.80\n",
      "Elastic Net MAPE: 21.28\n",
      "------------------------------ Random Forest\n",
      "Random Forest MSE: 0.16\n",
      "Random Forest R2: 0.85\n",
      "Random Forest MAPE: 16.83\n",
      "------------------------------ XGBoost\n",
      "XGBoost MSE: 0.16\n",
      "XGBoost R2: 0.85\n",
      "XGBoost MAPE: 17.61\n",
      "-------------------------------------------------- stats for :  by_reg --------------------------------------------------\n",
      "------------------------------ Linear Regression\n",
      "Linear Regression MSE: 0.18\n",
      "Linear Regression R2: 0.83\n",
      "Linear Regression MAPE: 21.47\n",
      "------------------------------ Elastic Net\n",
      "Elastic Net MSE: 0.20\n",
      "Elastic Net R2: 0.81\n",
      "Elastic Net MAPE: 21.29\n",
      "------------------------------ Random Forest\n",
      "Random Forest MSE: 0.15\n",
      "Random Forest R2: 0.85\n",
      "Random Forest MAPE: 16.94\n",
      "------------------------------ XGBoost\n",
      "XGBoost MSE: 0.16\n",
      "XGBoost R2: 0.85\n",
      "XGBoost MAPE: 17.66\n"
     ]
    }
   ],
   "source": [
    "# Initialisez best_models pour stocker √† la fois le meilleur estimateur et le meilleur score pour chaque mod√®le\n",
    "best_models = {name: {'model': None, 'score': {'mse': np.inf, 'r2': np.inf, 'mape': np.inf}, 'df': None} for name in models_params.keys()}\n",
    "\n",
    "for df_name, df in df_dict.items():\n",
    "    X = df.drop('valeur_fonciere', axis=1)\n",
    "    y = df['valeur_fonciere'] # Tr√®√®√®√®√®√®√®s important\n",
    "    \n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_features_indices = [i for i, col in enumerate(X.columns) if col in categorical_cols]\n",
    "\n",
    "    X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(50*'-', 'stats for : ', df_name, 50*'-')\n",
    "\n",
    "    for name, mp in models_params.items():\n",
    "        \n",
    "        print(30*'-', name)\n",
    "\n",
    "        model = clone(mp['model']) # On s'assure que la validation crois√©e commencera avec un mod√®le non entra√Æn√©\n",
    "\n",
    "        if name == 'CatBoost':\n",
    "            X_train, X_test, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            model.set_params(**{'cat_features': categorical_features_indices})\n",
    "            params = mp['params']\n",
    "        else:\n",
    "            params = mp['params']\n",
    "\n",
    "        grid = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        score = grid.score(X_test, y_test)\n",
    "        \n",
    "        y_pred = grid.best_estimator_.predict(X_test)\n",
    "        # Scores additionnels visant √† avoir d'autres perspectives\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        modified_mape = np.mean(np.abs((y_test - y_pred) * y_std / (y_test*y_std + y_mean))) * 100\n",
    "\n",
    "        print(f\"{name} MSE: {-score:.2f}\")\n",
    "        print(f\"{name} R2: {r2:.2f}\")\n",
    "        print(f\"{name} MAPE: {modified_mape:.2f}\")\n",
    "        \n",
    "        best_models[name] = {'model': grid.best_estimator_, \n",
    "                                'score': {'mse': -score, \n",
    "                                        'r2': r2, \n",
    "                                        'mape': modified_mape}, \n",
    "                            'df': df_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et l√†, nous sommes tr√®s d√©√ßus. Pourquoi? Parce que comme vu pr√©c√©demment sur les variables s√©lectionn√©es, la valeur fonci√®re d√©pend surtout de la surface du b√¢ti, c'est m√™me la feature jug√©e la plus importante par notre premier mod√®le catboost, ainsi que le prix moyen des voisins qui semble avoir une corr√©lation lin√©aire avec la valeur fonci√®re... Ainsi nos mod√®les de boosting fonctionnent (sur les variables s√©lectionn√©es) pas bien mieux (du point de vue de la MSE) qu'une simple r√©gression lin√©aire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.033295341776352"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On g√©n√®re des donn√©es d'exemple\n",
    "np.random.seed(0)\n",
    "y = np.random.normal(0, 1, 1000)  \n",
    "\n",
    "y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
    "\n",
    "# On prend des pr√©dictions y_pred qui sont 10% inf√©rieures √† y_test\n",
    "y_pred = y_test * 0.9\n",
    "\n",
    "y_mean = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "\n",
    "# Testons notre m√©trique!\n",
    "modified_mape = np.mean(np.abs((y_test - y_pred) * y_std / (y_test*y_std + y_mean))) * 100\n",
    "\n",
    "modified_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le MAPE modifi√© semble faire bon effet m√™me s'il surestime un peu les √©carts (ce qui n'est pas si mal)! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
