{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from EDA_tools import DfAnalysis, DataPreprocessing\n",
    "from DimRed_plus import PCAAnalysis\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score \n",
    "\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "agence = pd.read_csv(r\"data\\agence.csv\")\n",
    "france = pd.read_csv(r\"data\\france.csv\")\n",
    "insee_commune = pd.read_csv(r\"data\\insee_commune.csv\")\n",
    "localisation_commune = pd.read_csv(r\"data\\localisation_commune.csv\")\n",
    "map_code_insee = pd.read_csv(r\"data\\map_code_insee.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du dataset d'étude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous disposons de plusieurs datasets (agence, france, insee_commune, localisation), il serait intéressant de les combiner de telle sorte à pouvoir en extraire les données essentielles lors d'une EDA plus approfondie mais surtout afin de le fournir à nos modèles de machine learning que nous implémenterons dans un second jupyter notebook.\n",
    "\n",
    "Nous utiliserons notamment deux fichiers `DimRed_plus` et `EDA_tools` qui proposent certaines fonctionnalités d'automatisation de graphiques, d'étapes de preprocessing des données etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire\n",
    "\n",
    "Au cours de ce notebook Jupyter, nous entreprendrons les étapes suivantes pour enrichir notre jeu de données initial (`agence.csv`) avec des informations provenant d'autres fichiers afin de mieux correspondre au profil client et d'étudier l'impact de la performance des agents immobiliers sur la détermination de la valeur foncière des biens immobiliers en Haute-Garonne :\n",
    "\n",
    "1. **Analyse rapide du fichier agence.csv** :\n",
    "   - Exploration initiale des données pour comprendre leur structure et leur contenu.\n",
    "\n",
    "2. **Fusion avec le fichier france.csv** :\n",
    "   - Intégration des données sur la France pour enrichir notre jeu de données.\n",
    "\n",
    "3. **Analyse du fichier insee_commune.csv** :\n",
    "   - Exploration des données de l'INSEE sur les communes pour obtenir des informations supplémentaires.\n",
    "\n",
    "4. **Fusion entre les fichiers insee_commune.csv et localisation_commune.csv** :\n",
    "   - Combinaison des données de localisation des communes avec les données de l'INSEE pour une meilleure compréhension géographique.\n",
    "\n",
    "5. **Analyse des corrélations entre les variables** :\n",
    "   - Identification des relations et des dépendances entre les différentes caractéristiques du jeu de données.\n",
    "\n",
    "6. **Sélection manuelle des variables** :\n",
    "   - Choix des variables pertinentes à inclure dans notre modèle.\n",
    "\n",
    "7. **Préparation d'un jeu de données pour le machine learning** :\n",
    "   - Transformation des données en un format adapté à l'apprentissage automatique.\n",
    "\n",
    "8. **Test avec un premier modèle CatBoost** :\n",
    "   - Mise en œuvre d'un modèle de prédiction basé sur l'algorithme CatBoost pour évaluer sa performance dans la prédiction de la valeur foncière des biens immobiliers.\n",
    "\n",
    "À travers ces étapes, nous visons à améliorer la qualité de notre jeu de données, à identifier les facteurs clés influençant la valeur foncière et à construire un modèle prédictif robuste pour soutenir nos analyses futures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapes 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 13 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   date_mutation              4000 non-null   object \n",
      " 1   valeur_fonciere            4000 non-null   float64\n",
      " 2   code_postal                4000 non-null   int64  \n",
      " 3   nom_commune                4000 non-null   object \n",
      " 4   id_parcelle                4000 non-null   object \n",
      " 5   type_local                 4000 non-null   object \n",
      " 6   surface_reelle_bati        4000 non-null   float64\n",
      " 7   surface_terrain            4000 non-null   float64\n",
      " 8   nombre_pieces_principales  4000 non-null   float64\n",
      " 9   agent                      4000 non-null   int64  \n",
      " 10  prixcible                  4000 non-null   int64  \n",
      " 11  prixvente_initial          4000 non-null   int64  \n",
      " 12  evaluation_agent           4000 non-null   object \n",
      "dtypes: float64(4), int64(4), object(5)\n",
      "memory usage: 406.4+ KB\n"
     ]
    }
   ],
   "source": [
    "agence.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps:\n",
    "- Nous effectuons la conversion de type des dates \n",
    "- Nous vérifions les valeurs manquantes dans le dataset en créant une instance de `DfAnalysis`\n",
    "    - Nous ajouterons le préfixe 'dan' pour différencier cette instance du dataset original\n",
    "\n",
    "Note: l'étude complète (outliers, corrélations...) du dataset interviendra plus tard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agence['date_mutation'] = pd.to_datetime(agence['date_mutation'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 row(s) have 0 missing values\n",
      "Total number of rows with missing values: 0\n",
      "List of indexes of rows with missing values: []\n",
      "--------------------------------------------------\n",
      "No duplicated rows\n"
     ]
    }
   ],
   "source": [
    "dan_agence = DfAnalysis(agence)\n",
    "dan_agence.missing_values_check()\n",
    "print(50*'-')\n",
    "dan_agence.duplicate_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un second temps: \n",
    "- Nous ne sélectionnons que la plage de données de `france.csv` concernant la haute garonne\n",
    "\n",
    "`france.csv` est un dataset intéressant en ce qu'il réunit des informations relatives au carreau d'appartenance des biens considérés (un carreau est une unité de 200m de côté sur laquelle peuvent être considérées des variables comme la densité de population, le niveau de vie des ménages etc...): ceci en fait un dataset très fin et dons utile pour notre étude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_mutation</th>\n",
       "      <th>valeur_fonciere</th>\n",
       "      <th>adresse_code_voie</th>\n",
       "      <th>code_postal</th>\n",
       "      <th>nom_commune</th>\n",
       "      <th>code_commune</th>\n",
       "      <th>id_parcelle</th>\n",
       "      <th>type_local</th>\n",
       "      <th>surface_reelle_bati</th>\n",
       "      <th>surface_terrain</th>\n",
       "      <th>...</th>\n",
       "      <th>carreau_Ind_11_17</th>\n",
       "      <th>carreau_Ind_18_24</th>\n",
       "      <th>carreau_Ind_25_39</th>\n",
       "      <th>carreau_Ind_40_54</th>\n",
       "      <th>carreau_Ind_55_64</th>\n",
       "      <th>carreau_Ind_65_79</th>\n",
       "      <th>carreau_Ind_80p</th>\n",
       "      <th>carreau_Ind_inc</th>\n",
       "      <th>carreau_I_pauv</th>\n",
       "      <th>carreau_t_maille</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>621964</th>\n",
       "      <td>2015-08-18</td>\n",
       "      <td>84000.0</td>\n",
       "      <td>3044</td>\n",
       "      <td>31200</td>\n",
       "      <td>Toulouse</td>\n",
       "      <td>31555</td>\n",
       "      <td>31555830AM0276</td>\n",
       "      <td>Appartement</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>271.9</td>\n",
       "      <td>322.0</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>867.0</td>\n",
       "      <td>348.1</td>\n",
       "      <td>301.0</td>\n",
       "      <td>112.1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662173</th>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>206500.0</td>\n",
       "      <td>0018</td>\n",
       "      <td>31470</td>\n",
       "      <td>Fontenilles</td>\n",
       "      <td>31188</td>\n",
       "      <td>311880000B0710</td>\n",
       "      <td>Maison</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1770.0</td>\n",
       "      <td>...</td>\n",
       "      <td>124.6</td>\n",
       "      <td>54.6</td>\n",
       "      <td>114.0</td>\n",
       "      <td>244.2</td>\n",
       "      <td>79.0</td>\n",
       "      <td>64.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       date_mutation  valeur_fonciere adresse_code_voie code_postal  \\\n",
       "621964    2015-08-18          84000.0              3044       31200   \n",
       "662173    2017-12-14         206500.0              0018       31470   \n",
       "\n",
       "        nom_commune code_commune     id_parcelle   type_local  \\\n",
       "621964     Toulouse        31555  31555830AM0276  Appartement   \n",
       "662173  Fontenilles        31188  311880000B0710       Maison   \n",
       "\n",
       "        surface_reelle_bati  surface_terrain  ...  carreau_Ind_11_17  \\\n",
       "621964                 50.0             50.0  ...              271.9   \n",
       "662173                 83.0           1770.0  ...              124.6   \n",
       "\n",
       "        carreau_Ind_18_24  carreau_Ind_25_39  carreau_Ind_40_54  \\\n",
       "621964              322.0             1562.0              867.0   \n",
       "662173               54.6              114.0              244.2   \n",
       "\n",
       "        carreau_Ind_55_64 carreau_Ind_65_79  carreau_Ind_80p  carreau_Ind_inc  \\\n",
       "621964              348.1             301.0            112.1             77.0   \n",
       "662173               79.0              64.9             17.0             31.0   \n",
       "\n",
       "        carreau_I_pauv  carreau_t_maille  \n",
       "621964               0              1000  \n",
       "662173               0              1000  \n",
       "\n",
       "[2 rows x 46 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "france['code_postal'] = france['code_postal'].astype(str)\n",
    "france_HG = france[france['code_postal'].str.startswith('31')]\n",
    "france_HG.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france_HG (57661, 46) | agence (4000, 13)\n"
     ]
    }
   ],
   "source": [
    "print('france_HG',france_HG.shape, '|', 'agence', agence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous effectuons quelques conversions de type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "france_HG['date_mutation'] = pd.to_datetime(france_HG['date_mutation'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate ici que les coordonnées géographiques de `france.csv` concernent les biens eux-mêmes cela peut nous donner des idées de combinaisons de features interessantes pour l'avenir:\n",
    "- Distance du bien au centre ville de sa commune d'appartenance en utilisant `localisation_commune.csv`\n",
    "- Distance du bien au centre ville de Toulouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nom_commune</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42517</th>\n",
       "      <td>Bourbon-l'Archambault</td>\n",
       "      <td>46.600380</td>\n",
       "      <td>3.070512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42518</th>\n",
       "      <td>Bourbon-l'Archambault</td>\n",
       "      <td>46.597794</td>\n",
       "      <td>3.080006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42918</th>\n",
       "      <td>Bourbon-l'Archambault</td>\n",
       "      <td>46.586081</td>\n",
       "      <td>3.060649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42919</th>\n",
       "      <td>Bourbon-l'Archambault</td>\n",
       "      <td>46.585826</td>\n",
       "      <td>3.066112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42920</th>\n",
       "      <td>Bourbon-l'Archambault</td>\n",
       "      <td>46.579173</td>\n",
       "      <td>3.055870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 nom_commune   latitude  longitude\n",
       "42517  Bourbon-l'Archambault  46.600380   3.070512\n",
       "42518  Bourbon-l'Archambault  46.597794   3.080006\n",
       "42918  Bourbon-l'Archambault  46.586081   3.060649\n",
       "42919  Bourbon-l'Archambault  46.585826   3.066112\n",
       "42920  Bourbon-l'Archambault  46.579173   3.055870"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "france_HG[['nom_commune', 'latitude', 'longitude']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2641120 entries, 0 to 2641119\n",
      "Data columns (total 46 columns):\n",
      " #   Column                     Dtype  \n",
      "---  ------                     -----  \n",
      " 0   date_mutation              object \n",
      " 1   valeur_fonciere            float64\n",
      " 2   adresse_code_voie          object \n",
      " 3   code_postal                object \n",
      " 4   nom_commune                object \n",
      " 5   code_commune               object \n",
      " 6   id_parcelle                object \n",
      " 7   type_local                 object \n",
      " 8   surface_reelle_bati        float64\n",
      " 9   surface_terrain            float64\n",
      " 10  nombre_pieces_principales  float64\n",
      " 11  longitude                  float64\n",
      " 12  latitude                   float64\n",
      " 13  departement                int64  \n",
      " 14  index_right                int64  \n",
      " 15  carreau_Id_carr_n          object \n",
      " 16  carreau_Ind                float64\n",
      " 17  carreau_Men                float64\n",
      " 18  carreau_Men_pauv           float64\n",
      " 19  carreau_Men_1ind           float64\n",
      " 20  carreau_Men_5ind           float64\n",
      " 21  carreau_Men_prop           float64\n",
      " 22  carreau_Men_fmp            float64\n",
      " 23  carreau_Ind_snv            float64\n",
      " 24  carreau_Men_surf           float64\n",
      " 25  carreau_Men_coll           float64\n",
      " 26  carreau_Men_mais           float64\n",
      " 27  carreau_Log_av45           float64\n",
      " 28  carreau_Log_45_70          float64\n",
      " 29  carreau_Log_70_90          float64\n",
      " 30  carreau_Log_ap90           float64\n",
      " 31  carreau_Log_inc            float64\n",
      " 32  carreau_Log_soc            float64\n",
      " 33  carreau_Ind_0_3            float64\n",
      " 34  carreau_Ind_4_5            float64\n",
      " 35  carreau_Ind_6_10           float64\n",
      " 36  carreau_Ind_11_17          float64\n",
      " 37  carreau_Ind_18_24          float64\n",
      " 38  carreau_Ind_25_39          float64\n",
      " 39  carreau_Ind_40_54          float64\n",
      " 40  carreau_Ind_55_64          float64\n",
      " 41  carreau_Ind_65_79          float64\n",
      " 42  carreau_Ind_80p            float64\n",
      " 43  carreau_Ind_inc            float64\n",
      " 44  carreau_I_pauv             int64  \n",
      " 45  carreau_t_maille           int64  \n",
      "dtypes: float64(34), int64(4), object(8)\n",
      "memory usage: 926.9+ MB\n"
     ]
    }
   ],
   "source": [
    "france.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de s'assurer de la correspondance parfaite entre les ventes réalisées par l'agence et celles entrées dans `france.csv` nous effectuons un inner merge sur l'identifiant de la parcelle concernée et sur la date de l'opération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 57)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enriched1 = pd.merge(agence, france_HG, how='inner', on=['id_parcelle', 'date_mutation'])\n",
    "df_enriched1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes communes sont désormais préfixées, il convient de retirer ces préfixes et de supprimer les colonnes dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_enriched1['nom_commune_x'] == df_enriched1['nom_commune_y']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 rows that are duplicated so we need to drop them\n",
      "After dropping duplicated rows, there are 51 rows left\n"
     ]
    }
   ],
   "source": [
    "temp_df_enriched1 = DfAnalysis(df_enriched1.T)\n",
    "temp_df_enriched1.duplicate_check()\n",
    "df_enriched1 = temp_df_enriched1.df.T\n",
    "\n",
    "df_enriched1.rename(columns={col: col.rsplit('_', 1)[0] for col in df_enriched1.columns if col.endswith('_x')}, inplace=True)\n",
    "\n",
    "df_enriched1[['code_postal', 'code_commune']] # considérons code commune qui n'équivaut pas au code_postal\n",
    "df_enriched1['code_commune'] = df_enriched1['code_commune'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">ATTENTION</font>: `code_commune` ne correspond pas au code postal de la commune mais à l'identifiant attribué par l'Insee à chaque commune de France\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapes 3, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons encore ajouter les variables de géolocalisation des communes (elles remplaceront leurs noms et leur code postal puisqu'elle sont uniques) et nous permettront de calculer la distance au centre ville de la commune de rattachement du bien ainsi que la distance au centre ville de toulouse (deux nouvelles features). \n",
    "\n",
    "- `insee_commune.csv` est un dataset essentiel pour notre étude car il contient des informations socio économiques relatives aux communes que l'on considèrera\n",
    "- `localisation_commune.csv` est moins riche mais non moins intéressant: il contient notamment les coordonnées gps des centres ville des communes étudiées ici "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois on sélectionne la plage de données concernant la Haute-Garonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "localisation_commune['Code_postal'] = localisation_commune['Code_postal'].astype(str)\n",
    "mask = localisation_commune['Code_postal'].str.startswith('31')\n",
    "localisation_commune_HG = localisation_commune[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<class 'str'>], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "localisation_commune_HG['coordonnees_gps'].apply(lambda s: type(s)).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous séparons la colonne `coordonnees_gps` en deux nouvelles colonnes `latitude_comm` et 'longitude_comm' pour préciser qu'il s'agit des coordonnées du centre ville de la commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       latitude_comm  longitude_comm\n",
      "269        46.460771        2.559048\n",
      "270        46.219967        3.249307\n",
      "273        46.401575        2.730717\n",
      "281        46.266258        3.229794\n",
      "286        46.269689        3.423880\n",
      "...              ...             ...\n",
      "33986      43.663774        1.677386\n",
      "33987      43.750880        1.490977\n",
      "33988      43.406779        1.719651\n",
      "33989      43.828733        1.496849\n",
      "33990      43.134325        0.485144\n",
      "\n",
      "[690 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "localisation_commune_HG[['latitude_comm', 'longitude_comm']] = localisation_commune_HG['coordonnees_gps'].str.split(',', expand=True)\n",
    "\n",
    "localisation_commune_HG['latitude_comm'] = pd.to_numeric(localisation_commune_HG['latitude_comm'], errors='coerce')\n",
    "localisation_commune_HG['longitude_comm'] = pd.to_numeric(localisation_commune_HG['longitude_comm'], errors='coerce')\n",
    "\n",
    "print(localisation_commune_HG[['latitude_comm', 'longitude_comm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "localisation_commune_HG.rename(columns={'Code_commune_INSEE': 'code_commune'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous ne nous donnons pas la peine de sélectionner les communes de la Haute-Garonne de `insee_commune.csv`, nous les obtiendrons naturellement lorsque nous fusionneront ce dataset avec `localisation_commune_HG.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "insee_commune.rename(columns={'CODGEO':'code_commune'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous nous séparons des variables parasites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "localisation_commune_HG = localisation_commune_HG.drop(['Ligne_5',\n",
    "       'Libellé_d_acheminement', 'Libell_d_acheminement', 'coordonnees_gps'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous fusionnons nos deux datasets sur ce fameux `code_commune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched2 = pd.merge(insee_commune, localisation_commune_HG, how='inner', on='code_commune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous vérifions que nos deux datasets ne disposent pas d'autres colonnes en commun que la colonne de fusion afin d'éviter de potentiels problèmes de dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['code_commune'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_enriched2.columns[(df_enriched2.columns.isin(df_enriched1.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 dropping duplicate for code_commune:  (674, 40)\n",
      "df2:  (690, 40)\n"
     ]
    }
   ],
   "source": [
    "df_enriched2_unique = df_enriched2.drop_duplicates(subset=['code_commune'], keep='first')\n",
    "print('df2 dropping duplicate for code_commune: ', df_enriched2_unique.shape)\n",
    "print('df2: ', df_enriched2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class 'str'>]\n",
      "[<class 'str'>]\n"
     ]
    }
   ],
   "source": [
    "print(df_enriched1['code_commune'].apply(lambda s: type(s)).unique())\n",
    "print(df_enriched2['code_commune'].apply(lambda s: type(s)).unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que l'on peut bien se permettre de supprimer les index pour lesquels la valeur en `code_commune` est égale, il s'agit en fait que tout l'index est alors dupliqué, on peut donc le supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   code_commune       LIBGEO  REG DEP  P16_POP  P11_POP  SUPERF  NAIS1116  \\\n",
      "46        03158  Haut-Bocage   84  03      892      965   70.64        33   \n",
      "47        03158  Haut-Bocage   84  03      892      965   70.64        33   \n",
      "\n",
      "    DECE1116     P16_MEN  ...  ETFZ15  ETGU15  ETGZ15  ETOQ15  ETTEF115  \\\n",
      "46        29  361.206154  ...     8.0    34.0     5.0    10.0      17.0   \n",
      "47        29  361.206154  ...     8.0    34.0     5.0    10.0      17.0   \n",
      "\n",
      "    ETTEFP1015  Nom_commune  Code_postal  latitude_comm  longitude_comm  \n",
      "46         2.0  HAUT BOCAGE         3190      46.492864        2.658524  \n",
      "47         2.0  HAUT BOCAGE         3190      46.492864        2.658524  \n",
      "\n",
      "[2 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "doublons_mask = df_enriched2.duplicated(subset=['code_commune'], keep=False)\n",
    "duplicate_rows = df_enriched2.loc[doublons_mask]\n",
    "print(duplicate_rows.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons finalement notre dataset final en effectuant un merge left toujours sur le `code_commune` nous avons bien conservé la forme du dataset original, c'est bon signe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df_enriched1, df_enriched2_unique, how='left', on='code_commune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 90)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.shape # df_final a une forme correcte, égale à celle d'agence_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapes 5, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous devons éliminer quelques-unes des features qui ne sont pas pertinentes ou qui présentent des risques de data leakage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons 90 features\n"
     ]
    }
   ],
   "source": [
    "print(f'Nous avons {df_final.shape[1]} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_red = df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data leakage / index du df de droite\n",
    "df_final_red.drop(['prixcible', 'prixvente_initial', 'index_right'], axis=1, inplace=True) \n",
    "\n",
    "# features non pertinentes\n",
    "df_final_red.drop(\n",
    "    \n",
    "    ['adresse_code_voie', 'code_commune','code_postal', 'nom_commune',\n",
    "       'id_parcelle','LIBGEO', 'REG', 'DEP', 'Nom_commune','Code_postal', \n",
    "       'departement']\n",
    "       \n",
    "       , axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_mutation', 'valeur_fonciere', 'type_local', 'surface_reelle_bati',\n",
       "       'surface_terrain', 'nombre_pieces_principales', 'agent',\n",
       "       'evaluation_agent', 'code_postal_y', 'longitude', 'latitude',\n",
       "       'carreau_Id_carr_n', 'carreau_Ind', 'carreau_Men', 'carreau_Men_pauv',\n",
       "       'carreau_Men_1ind', 'carreau_Men_5ind', 'carreau_Men_prop',\n",
       "       'carreau_Men_fmp', 'carreau_Ind_snv', 'carreau_Men_surf',\n",
       "       'carreau_Men_coll', 'carreau_Men_mais', 'carreau_Log_av45',\n",
       "       'carreau_Log_45_70', 'carreau_Log_70_90', 'carreau_Log_ap90',\n",
       "       'carreau_Log_inc', 'carreau_Log_soc', 'carreau_Ind_0_3',\n",
       "       'carreau_Ind_4_5', 'carreau_Ind_6_10', 'carreau_Ind_11_17',\n",
       "       'carreau_Ind_18_24', 'carreau_Ind_25_39', 'carreau_Ind_40_54',\n",
       "       'carreau_Ind_55_64', 'carreau_Ind_65_79', 'carreau_Ind_80p',\n",
       "       'carreau_Ind_inc', 'carreau_I_pauv', 'carreau_t_maille', 'P16_POP',\n",
       "       'P11_POP', 'SUPERF', 'NAIS1116', 'DECE1116', 'P16_MEN', 'NAISD18',\n",
       "       'DECESD18', 'P16_LOG', 'P16_RP', 'P16_RSECOCC', 'P16_LOGVAC',\n",
       "       'P16_RP_PROP', 'NBMENFISC16', 'PIMP16', 'MED16', 'TP6016', 'P16_EMPLT',\n",
       "       'P16_EMPLT_SAL', 'P11_EMPLT', 'P16_POP1564', 'P16_CHOM1564',\n",
       "       'P16_ACT1564', 'ETTOT15', 'ETAZ15', 'ETBE15', 'ETFZ15', 'ETGU15',\n",
       "       'ETGZ15', 'ETOQ15', 'ETTEF115', 'ETTEFP1015', 'latitude_comm',\n",
       "       'longitude_comm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_red.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données statistiques des carreaux sont des données numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "carreau_to_numeric = [col for col in df_final_red.columns if col.startswith('carreau')]\n",
    "\n",
    "df_final_red[carreau_to_numeric] = df_final_red[carreau_to_numeric].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_red['date_mutation'] = pd.to_datetime(df_final_red['date_mutation'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 76 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   date_mutation              4000 non-null   datetime64[ns]\n",
      " 1   valeur_fonciere            4000 non-null   object        \n",
      " 2   type_local                 4000 non-null   object        \n",
      " 3   surface_reelle_bati        4000 non-null   object        \n",
      " 4   surface_terrain            4000 non-null   object        \n",
      " 5   nombre_pieces_principales  4000 non-null   object        \n",
      " 6   agent                      4000 non-null   object        \n",
      " 7   evaluation_agent           4000 non-null   object        \n",
      " 8   code_postal_y              4000 non-null   object        \n",
      " 9   longitude                  4000 non-null   object        \n",
      " 10  latitude                   4000 non-null   object        \n",
      " 11  carreau_Id_carr_n          0 non-null      float64       \n",
      " 12  carreau_Ind                4000 non-null   float64       \n",
      " 13  carreau_Men                4000 non-null   float64       \n",
      " 14  carreau_Men_pauv           4000 non-null   float64       \n",
      " 15  carreau_Men_1ind           4000 non-null   float64       \n",
      " 16  carreau_Men_5ind           4000 non-null   float64       \n",
      " 17  carreau_Men_prop           4000 non-null   float64       \n",
      " 18  carreau_Men_fmp            4000 non-null   float64       \n",
      " 19  carreau_Ind_snv            4000 non-null   float64       \n",
      " 20  carreau_Men_surf           4000 non-null   float64       \n",
      " 21  carreau_Men_coll           4000 non-null   float64       \n",
      " 22  carreau_Men_mais           4000 non-null   float64       \n",
      " 23  carreau_Log_av45           4000 non-null   float64       \n",
      " 24  carreau_Log_45_70          4000 non-null   float64       \n",
      " 25  carreau_Log_70_90          4000 non-null   float64       \n",
      " 26  carreau_Log_ap90           4000 non-null   float64       \n",
      " 27  carreau_Log_inc            4000 non-null   float64       \n",
      " 28  carreau_Log_soc            4000 non-null   float64       \n",
      " 29  carreau_Ind_0_3            4000 non-null   float64       \n",
      " 30  carreau_Ind_4_5            4000 non-null   float64       \n",
      " 31  carreau_Ind_6_10           4000 non-null   float64       \n",
      " 32  carreau_Ind_11_17          4000 non-null   float64       \n",
      " 33  carreau_Ind_18_24          4000 non-null   float64       \n",
      " 34  carreau_Ind_25_39          4000 non-null   float64       \n",
      " 35  carreau_Ind_40_54          4000 non-null   float64       \n",
      " 36  carreau_Ind_55_64          4000 non-null   float64       \n",
      " 37  carreau_Ind_65_79          4000 non-null   float64       \n",
      " 38  carreau_Ind_80p            4000 non-null   float64       \n",
      " 39  carreau_Ind_inc            4000 non-null   float64       \n",
      " 40  carreau_I_pauv             4000 non-null   int64         \n",
      " 41  carreau_t_maille           4000 non-null   int64         \n",
      " 42  P16_POP                    4000 non-null   int64         \n",
      " 43  P11_POP                    4000 non-null   int64         \n",
      " 44  SUPERF                     4000 non-null   float64       \n",
      " 45  NAIS1116                   4000 non-null   int64         \n",
      " 46  DECE1116                   4000 non-null   int64         \n",
      " 47  P16_MEN                    4000 non-null   float64       \n",
      " 48  NAISD18                    4000 non-null   int64         \n",
      " 49  DECESD18                   4000 non-null   int64         \n",
      " 50  P16_LOG                    4000 non-null   float64       \n",
      " 51  P16_RP                     4000 non-null   float64       \n",
      " 52  P16_RSECOCC                4000 non-null   float64       \n",
      " 53  P16_LOGVAC                 4000 non-null   float64       \n",
      " 54  P16_RP_PROP                4000 non-null   float64       \n",
      " 55  NBMENFISC16                4000 non-null   float64       \n",
      " 56  PIMP16                     3957 non-null   float64       \n",
      " 57  MED16                      4000 non-null   float64       \n",
      " 58  TP6016                     3895 non-null   float64       \n",
      " 59  P16_EMPLT                  4000 non-null   float64       \n",
      " 60  P16_EMPLT_SAL              4000 non-null   float64       \n",
      " 61  P11_EMPLT                  4000 non-null   float64       \n",
      " 62  P16_POP1564                4000 non-null   float64       \n",
      " 63  P16_CHOM1564               4000 non-null   float64       \n",
      " 64  P16_ACT1564                4000 non-null   float64       \n",
      " 65  ETTOT15                    4000 non-null   float64       \n",
      " 66  ETAZ15                     4000 non-null   float64       \n",
      " 67  ETBE15                     4000 non-null   float64       \n",
      " 68  ETFZ15                     4000 non-null   float64       \n",
      " 69  ETGU15                     4000 non-null   float64       \n",
      " 70  ETGZ15                     4000 non-null   float64       \n",
      " 71  ETOQ15                     4000 non-null   float64       \n",
      " 72  ETTEF115                   4000 non-null   float64       \n",
      " 73  ETTEFP1015                 4000 non-null   float64       \n",
      " 74  latitude_comm              4000 non-null   float64       \n",
      " 75  longitude_comm             4000 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(57), int64(8), object(10)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final_red.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutes les valeurs de `carreau_Id_carr_n` sont nulles, on supprime donc cette colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_red.drop('carreau_Id_carr_n', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous convertissons enfin les variables restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_numeric = ['valeur_fonciere', 'surface_reelle_bati',\n",
    "                    'surface_terrain', 'nombre_pieces_principales',\n",
    "                    'longitude', 'latitude']\n",
    "\n",
    "df_final_red[cols_to_numeric] = df_final[cols_to_numeric].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 75 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   date_mutation              4000 non-null   datetime64[ns]\n",
      " 1   valeur_fonciere            4000 non-null   float64       \n",
      " 2   type_local                 4000 non-null   object        \n",
      " 3   surface_reelle_bati        4000 non-null   float64       \n",
      " 4   surface_terrain            4000 non-null   float64       \n",
      " 5   nombre_pieces_principales  4000 non-null   float64       \n",
      " 6   agent                      4000 non-null   object        \n",
      " 7   evaluation_agent           4000 non-null   object        \n",
      " 8   code_postal_y              4000 non-null   object        \n",
      " 9   longitude                  4000 non-null   float64       \n",
      " 10  latitude                   4000 non-null   float64       \n",
      " 11  carreau_Ind                4000 non-null   float64       \n",
      " 12  carreau_Men                4000 non-null   float64       \n",
      " 13  carreau_Men_pauv           4000 non-null   float64       \n",
      " 14  carreau_Men_1ind           4000 non-null   float64       \n",
      " 15  carreau_Men_5ind           4000 non-null   float64       \n",
      " 16  carreau_Men_prop           4000 non-null   float64       \n",
      " 17  carreau_Men_fmp            4000 non-null   float64       \n",
      " 18  carreau_Ind_snv            4000 non-null   float64       \n",
      " 19  carreau_Men_surf           4000 non-null   float64       \n",
      " 20  carreau_Men_coll           4000 non-null   float64       \n",
      " 21  carreau_Men_mais           4000 non-null   float64       \n",
      " 22  carreau_Log_av45           4000 non-null   float64       \n",
      " 23  carreau_Log_45_70          4000 non-null   float64       \n",
      " 24  carreau_Log_70_90          4000 non-null   float64       \n",
      " 25  carreau_Log_ap90           4000 non-null   float64       \n",
      " 26  carreau_Log_inc            4000 non-null   float64       \n",
      " 27  carreau_Log_soc            4000 non-null   float64       \n",
      " 28  carreau_Ind_0_3            4000 non-null   float64       \n",
      " 29  carreau_Ind_4_5            4000 non-null   float64       \n",
      " 30  carreau_Ind_6_10           4000 non-null   float64       \n",
      " 31  carreau_Ind_11_17          4000 non-null   float64       \n",
      " 32  carreau_Ind_18_24          4000 non-null   float64       \n",
      " 33  carreau_Ind_25_39          4000 non-null   float64       \n",
      " 34  carreau_Ind_40_54          4000 non-null   float64       \n",
      " 35  carreau_Ind_55_64          4000 non-null   float64       \n",
      " 36  carreau_Ind_65_79          4000 non-null   float64       \n",
      " 37  carreau_Ind_80p            4000 non-null   float64       \n",
      " 38  carreau_Ind_inc            4000 non-null   float64       \n",
      " 39  carreau_I_pauv             4000 non-null   int64         \n",
      " 40  carreau_t_maille           4000 non-null   int64         \n",
      " 41  P16_POP                    4000 non-null   int64         \n",
      " 42  P11_POP                    4000 non-null   int64         \n",
      " 43  SUPERF                     4000 non-null   float64       \n",
      " 44  NAIS1116                   4000 non-null   int64         \n",
      " 45  DECE1116                   4000 non-null   int64         \n",
      " 46  P16_MEN                    4000 non-null   float64       \n",
      " 47  NAISD18                    4000 non-null   int64         \n",
      " 48  DECESD18                   4000 non-null   int64         \n",
      " 49  P16_LOG                    4000 non-null   float64       \n",
      " 50  P16_RP                     4000 non-null   float64       \n",
      " 51  P16_RSECOCC                4000 non-null   float64       \n",
      " 52  P16_LOGVAC                 4000 non-null   float64       \n",
      " 53  P16_RP_PROP                4000 non-null   float64       \n",
      " 54  NBMENFISC16                4000 non-null   float64       \n",
      " 55  PIMP16                     3957 non-null   float64       \n",
      " 56  MED16                      4000 non-null   float64       \n",
      " 57  TP6016                     3895 non-null   float64       \n",
      " 58  P16_EMPLT                  4000 non-null   float64       \n",
      " 59  P16_EMPLT_SAL              4000 non-null   float64       \n",
      " 60  P11_EMPLT                  4000 non-null   float64       \n",
      " 61  P16_POP1564                4000 non-null   float64       \n",
      " 62  P16_CHOM1564               4000 non-null   float64       \n",
      " 63  P16_ACT1564                4000 non-null   float64       \n",
      " 64  ETTOT15                    4000 non-null   float64       \n",
      " 65  ETAZ15                     4000 non-null   float64       \n",
      " 66  ETBE15                     4000 non-null   float64       \n",
      " 67  ETFZ15                     4000 non-null   float64       \n",
      " 68  ETGU15                     4000 non-null   float64       \n",
      " 69  ETGZ15                     4000 non-null   float64       \n",
      " 70  ETOQ15                     4000 non-null   float64       \n",
      " 71  ETTEF115                   4000 non-null   float64       \n",
      " 72  ETTEFP1015                 4000 non-null   float64       \n",
      " 73  latitude_comm              4000 non-null   float64       \n",
      " 74  longitude_comm             4000 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(62), int64(8), object(4)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_final_red.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sélection de variable**\n",
    "\n",
    "Nous commençons tout d'abord par générer la matrice des corrélations:\n",
    "- (voir dossier img $\\rightarrow$ correlation_matrix.png)\n",
    "- Nous remarquons que de très nombreuses variables sont très fortement corrélées, il y a donc matière à sélection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan_df_final_red = DfAnalysis(df_final_red)\n",
    "dan_df_final_red.correlation_matrix() # beaucoup de variables sont fortement corrélées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de sélectionner nos variables nous devons nous occuper des index pour lesquels des données manquent, nous pourrions: supprimer les index, remplir artificiellement les données manquantes avec filna s'il s'agit de données numériques ou supprimer les features concernées si celles-ci ne s'avèrent pas être d'un grand intérêt pour notre étude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PIMP16', 'TP6016']\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "cols_with_nan = df_final_red.loc[3786].isna()\n",
    "\n",
    "# Afficher les noms des colonnes qui ont une valeur NaN pour cet index\n",
    "print(cols_with_nan[cols_with_nan].index.tolist())\n",
    "\n",
    "df_final_clean = df_final_red\n",
    "\n",
    "df_final_clean.drop(['PIMP16', 'TP6016'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PIMP16`\n",
    "Part des ménages fiscaux imposés (%)\n",
    "\n",
    "`TP6016`\n",
    "Taux de pauvreté-Ensemble (%)\n",
    "\n",
    "On peut lire sur la matrice de corrélation que ces deux variables sont toutes deux très corrélées à au moins une autre variable (au delà de 75% en valeur absolue) on peut donc raisonnablement se permettre de les supprimer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_dossier = r'C:\\Users\\User\\Desktop\\Projets en cours\\Data_Science\\Mini-projets\\Case_cap_gemini_emmanuel_benichou\\df_ml'\n",
    "nom_fichier = 'df_for_ml.csv'\n",
    "chemin_complet = os.path.join(chemin_dossier, nom_fichier)\n",
    "\n",
    "os.makedirs(chemin_dossier, exist_ok=True)\n",
    "\n",
    "df_final_clean.to_csv(chemin_complet, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature engineering**\n",
    "\n",
    "Nous ajoutons à présent les distances que nous avions mentionnées tout à l'heure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ATTENTION le df est largement déséquilibré (voir le nombre de ventes à toulouse représentée ici par sa longitude et latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapes 7, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [col for col in df_final_clean.select_dtypes(include='object').columns]\n",
    "df_final_clean_ml = DataPreprocessing(df_final_clean, columns_to_exclude=['date_mutation'], categorical_features=cat_cols)\n",
    "df_final_clean_ml.scale_selected_columns()\n",
    "X_train, X_test, y_train, y_test = df_final_clean_ml.split_data('valeur_fonciere')\n",
    "ml_df = df_final_clean_ml.scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La PCA ne semble pas être concluante, elle combine de trop nombreuses variables corrélées pour avoir du sens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implémentation de notre premier modèle de machine learning**\n",
    "\n",
    "L'objectif ici est de tester notre dataset, d'essayer d'évaluer sans sélection préalable de variable le niveau d'overfitting d'un modèle performant pour la régression, nous utilisons catboost regressor, un algorithme de forêts d'arbres de décisions avec gradient boosting prenant plutôt bien en charge les hautes dimensionnalités ainsi que les variables catégorielles (au nombre de 4 dans notre dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    }
   ],
   "source": [
    "cat = CatBoostRegressor(loss_function='RMSE', cat_features=cat_cols, verbose=False)\n",
    "\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [100, 500]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=cat, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_cat = grid_search.best_estimator_\n",
    "y_pred = best_cat.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.15588928477376396\n",
      "----------------------------------------------------------------------------------------------------\n",
      "R2 score: 0.8463892029430315\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best CatBoost parameters: {'depth': 4, 'iterations': 500, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(100 * '-')\n",
    "print(\"R2 score:\", r2_score(y_test, y_pred))\n",
    "print(100 * '-')\n",
    "print('Best CatBoost parameters:', grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous atteignons une profondeur de 4, 500 apprenants faibles, un learning rate presque minimal ainsi qu'une excellente performance en terme de R2 score: tout ceci suggère très fortement de l'overfitting, il va donc falloir réduire notre dimensionnalité tout en veillant à préserver une certaine explicabilité de notre modèle. Nous testerons également plusieurs modèles en pipeline sur des jeux de features sélectionnées ou non afin d'établir une comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "                Feature  Importance\n",
      "0   surface_reelle_bati   57.595213\n",
      "1       surface_terrain   12.995068\n",
      "69     evaluation_agent    6.235640\n",
      "4              latitude    3.258643\n",
      "3             longitude    2.824106\n",
      "..                  ...         ...\n",
      "41              NAISD18    0.000000\n",
      "20      carreau_Log_inc    0.000000\n",
      "38             NAIS1116    0.000000\n",
      "39             DECE1116    0.000000\n",
      "35              P16_POP    0.000000\n",
      "\n",
      "[71 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "feature_importance = best_cat.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Note</font>:\n",
    "\n",
    "Néanmoins le modèle suggère ici une très grande importance de certaines variables, ce qui ne veut pas non plus dire grand chose du fait que de très nombreuses variables sont fortement corrélées ce qui peut nous conduire à une importance accrue de certaines variables au détriment d'autres (leur importance vient 'nourrir' artificiellement celle d'une seule, sélectionnée arbitrairement par le modèle).\n",
    "\n",
    "Ce n'est donc pas nécessairement une bonne manière de sélectionner des variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
