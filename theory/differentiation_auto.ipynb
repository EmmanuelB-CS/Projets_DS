{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphes de calculs: les bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous montrons ici comment on implémente un graphe de calcul et comment ce dernier peut être utilisé pour calculer des gradients de fonctions à plusieurs variables (variables réelles simples pour l'instant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noeud:\n",
    "    \"\"\"\n",
    "    Cette classe permet de réaliser le forward pass et le backward pass\n",
    "\n",
    "    La structure de cette classe est simple: on définiti des méthodes réalisant l'exponentiation, l'addition la multiplication\n",
    "    et qui instancie à chaque fois un noeud résultant avec pour attributs:\n",
    "    - Une valeur\n",
    "    - Des parents\n",
    "    - Un gradient\n",
    "    - Une fonction privée _backward attribut des noeuds créés par des opérations élémentaires\n",
    "\n",
    "    Note: \n",
    "    Les fonctions _backward ainsi définies seront appelées par la méthode backward \n",
    "    \"\"\"\n",
    "    def __init__(self, valeur, parents=[]):\n",
    "        self.valeur = valeur\n",
    "        self.parents = parents\n",
    "\n",
    "        self.grad = 0\n",
    "        self._backward = lambda : None \n",
    "\n",
    "    def __add__(self, b):\n",
    "        c = Noeud(self.valeur + b.valeur, parents=[self, b])\n",
    "\n",
    "        def _backward():\n",
    "            # On identifie self à a, on calcule donc le gradient pour a et b \n",
    "            self.grad += 1 * c.grad # += au cas ou les parents auraient plusieurs enfants\n",
    "            b.grad += 1 * c.grad\n",
    "        \n",
    "        c._backward = _backward\n",
    "\n",
    "        return c\n",
    "\n",
    "    def __mul__(self, b):\n",
    "        c = Noeud(self.valeur * b.valeur, parents=[self, b])\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += b.valeur * c.grad\n",
    "            b.grad += self.valeur * c.grad\n",
    "        \n",
    "        c._backward = _backward\n",
    "\n",
    "        return c\n",
    "\n",
    "    def exp(self):\n",
    "        c = Noeud(math.exp(self.valeur), parents=[self])\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += c.valeur * c.grad\n",
    "        \n",
    "        c._backward = _backward\n",
    "\n",
    "        return c\n",
    "    \n",
    "    def clear_grads(self):\n",
    "        a_visiter = self.parents.copy()\n",
    "        \n",
    "        while not len(a_visiter) == 0:\n",
    "            s = a_visiter.pop()\n",
    "            s.grad = 0\n",
    "        \n",
    "        for parent in s.parents:\n",
    "            if parent not in a_visiter:\n",
    "                a_visiter.append(parent)\n",
    "\n",
    "    def backward(self):\n",
    "        # remettre à 0 les gradients\n",
    "        self.clear_grads()\n",
    "\n",
    "        dejaVu = set()\n",
    "        L = []\n",
    "\n",
    "        def parcours(s):\n",
    "            dejaVu.add(s)\n",
    "            \n",
    "            for parent in s.parents:\n",
    "                if parent not in dejaVu:\n",
    "                    parcours(parent)\n",
    "\n",
    "                # fin traitement de s\n",
    "            L.append(s)\n",
    "                \n",
    "        parcours(self)\n",
    "\n",
    "        self.grad = 1 # Il s'agit du gradient de la sortie par rapport à elle même \n",
    "        for s in reversed(L):\n",
    "            s._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes importantes:\n",
    "---\n",
    "\n",
    "**Utilisation de += dans _backward**\n",
    "- L'opérateur += est utilisé pour mettre à jour le gradient de chaque parent (self et b ici) pendant la rétropropagation. La fonction _backward de c est définie pour augmenter le gradient de chaque parent par c.grad (le gradient du nœud actuel par rapport à la sortie du graphe) multiplié par la dérivée de la valeur de c par rapport à la valeur du parent, qui est 1 dans ce cas d'addition.\n",
    "\n",
    "- Pour self et b, le gradient est augmenté (+=) parce que chaque nœud peut avoir plusieurs enfants contribuant à son gradient total. L'opération += assure que chaque contribution (de chaque enfant dans le graphe) est cumulativement ajoutée au gradient du nœud parent.\n",
    "\n",
    "- Cette mise à jour est essentielle pour la rétropropagation, permettant à l'information du gradient de se propager à travers le graphe depuis la sortie vers l'entrée, pour que chaque nœud puisse ajuster sa valeur pour minimiser la fonction de coût lors de l'entraînement.\n",
    "En résumé, le += permet d'accumuler les gradients venant de différents chemins dans le graphe de calcul, ce qui est crucial pour la mise à jour correcte des poids dans les réseaux de neurones lors de la rétropropagation.\n",
    "\n",
    "**Fonctionnement de _backward()**\n",
    "- Lorsqu'on définit une fonction `_backward` à l'intérieur d'une méthode comme `__add__` ou `__mul__`, cette fonction est une fermeture (closure en anglais) qui capture et retient les variables locales de la méthode dans laquelle elle a été définie. En Python, une fermeture permet à une fonction interne de se souvenir de l'état de son environnement lorsque la fonction a été créée, même après que le bloc de code extérieur a été exécuté.\n",
    "\n",
    "- Dans notre cas, lorsque on définit `_backward` à l'intérieur de `__add__` ou `__mul__`, les variables `self`, `B`, et `C` sont capturées par `_backward`. Cela signifie que `_backward` a accès aux instances spécifiques de `self` (le noeud sur lequel l'opération a été appelée) et `B` (le noeud passé en argument à l'opération) qui étaient présentes au moment où l'opération a été effectuée. De plus, `C` est le nouveau noeud résultant de l'opération, et `_backward` est stocké dans cet objet `C`. C'est pourquoi vous pouvez utiliser `self`, `B`, et `C` dans votre fonction `_backward` pour calculer correctement les gradients lors de la rétropropagation.\n",
    "\n",
    "Voici un petit exemple pour illustrer comment `self` et `B` sont capturés par la fermeture `_backward` dans le contexte de la méthode `__add__` :\n",
    "\n",
    "```python\n",
    "class Noeud:\n",
    "    def __init__(self, valeur):\n",
    "        self.valeur = valeur\n",
    "        self.grad = 0  # Gradient initialisé à 0\n",
    "        self._backward = lambda: None  # Fonction backward vide par défaut\n",
    "\n",
    "    def __add__(self, autre):\n",
    "        resultat = Noeud(self.valeur + autre.valeur)  # Crée un nouveau noeud avec la somme des valeurs\n",
    "\n",
    "        def _backward():\n",
    "            # La fermeture '_backward' capture 'self' et 'autre' ici\n",
    "            self.grad += resultat.grad  # Met à jour le gradient de 'self'\n",
    "            autre.grad += resultat.grad  # Met à jour le gradient de 'autre'\n",
    "        \n",
    "        resultat._backward = _backward  # Stocke la fonction '_backward' dans le nouveau noeud résultant\n",
    "\n",
    "        return resultat\n",
    "```\n",
    "\n",
    "Quand on appelle `backward()` sur le noeud résultant, le processus de rétropropagation déclenche les fonctions `_backward` stockées dans chaque noeud, en commençant par le noeud de sortie et en remontant à travers le graphe de calcul. Chaque fonction `_backward` exécute le code qui a été défini lors de sa création, utilisant les instances de `self` et `B` (et éventuellement `C`) qui ont été capturées (c'est-à-dire mémorisées) par la fermeture. Cela permet de calculer et d'accumuler correctement les gradients à travers le graphe de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Noeud(3)\n",
    "b = Noeud(4)\n",
    "\n",
    "c = a * b\n",
    "d = c * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.backward()\n",
    "a.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implémentation de la Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extension à de nouvelles structures de données\n",
    "\n",
    "Au lieu de nous limiter uniquement à des valeurs scalaires, nous généralisons notre approche aux structures de données plus complexes. En particulier, le gradient d'un noeud par rapport à ses parents, noté `dA`, est représenté par une **matrice jacobienne** dans le cas général. Cette généralisation nous permet de manipuler efficacement des gradients pour des opérations impliquant des vecteurs, des matrices, ou des tenseurs de dimensions supérieures.\n",
    "\n",
    "#### Règles de Calcul du Gradient\n",
    "\n",
    "Le calcul des gradients varie en fonction de l'opération effectuée sur les noeuds. Voici les formules générales pour les opérations courantes :\n",
    "\n",
    "- **Pour une somme de deux noeuds** : La somme est une opération qui ne modifie pas le gradient. Ainsi, pour la somme de deux noeuds, nous avons :\n",
    "\n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial A} = \\frac{\\partial C}{\\partial B} = \\mathbf{dC}\n",
    "  $\n",
    "\n",
    "  où $\\mathbf{dC}$ représente le gradient de la sortie par rapport à chacun des noeuds entrants.\n",
    "\n",
    "- **Pour un produit de deux noeuds** : Le produit matriciel implique une interaction plus complexe entre les gradients. Les règles de dérivation pour un produit matriciel sont les suivantes :\n",
    "\n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial A} = \\mathbf{dC} \\cdot B^\\top\n",
    "  $\n",
    "  \n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial B} = A^\\top \\cdot \\mathbf{dC}\n",
    "  $\n",
    "\n",
    "  où $A^\\top$ et $B^\\top$ désignent les transposées des matrices $A$ et $B$, respectivement.\n",
    "\n",
    "- **Pour une transformation unaire** : Une transformation unaire $f$ appliquée à un noeud $A$ (par exemple, une fonction d'activation dans un réseau de neurones) aura un impact sur le gradient calculé selon la règle de la chaîne. Dans ce cas, le gradient est donné par :\n",
    "\n",
    "  $\n",
    "  \\frac{\\partial C}{\\partial A} = f'(A) \\cdot \\mathbf{dC}\n",
    "  $\n",
    "\n",
    "  où $f'(A)$ est la dérivée de la fonction $f$ évaluée au noeud $A$, et $\\mathbf{dC}$ est le gradient de la sortie par rapport à $A$.\n",
    "\n",
    "Ces formules constituent la base du calcul automatique des gradients dans les graphes de calcul différentiables.\n",
    "\n",
    "#### Maintenant reprenons notre code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noeud:\n",
    "    \"\"\"\n",
    "    Implémentation d'un noeud dans un graphe de calcul différentiable.\n",
    "    \n",
    "    Cette classe permet la création d'un graphe de calcul pour effectuer des opérations \n",
    "    élémentaires (comme l'addition, la multiplication, l'exponentiation, etc.) et calculer \n",
    "    leur gradient par rapport à leurs entrées, en utilisant le mécanisme de backpropagation.\n",
    "    \n",
    "    Attributs:\n",
    "        valeur (np.ndarray): La valeur calculée au noeud.\n",
    "        parents (list): Une liste de noeuds parents dont dépend la valeur de ce noeud.\n",
    "        grad (np.ndarray): Le gradient de la fonction de perte par rapport à ce noeud, initialisé à zéro.\n",
    "        _backward (function): Une fonction lambda privée qui sera définie par les opérations spécifiques pour calculer le gradient par rapport à ses parents.\n",
    "    \n",
    "    Méthodes:\n",
    "        __init__, __add__, __mul__, somme_lignes, somme_colonnes, somme, exp, clear_grads, backward: \n",
    "        Diverses opérations et mécanismes pour construire et manipuler le graphe de calcul, ainsi que pour effectuer le backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, valeur, parents=[]):\n",
    "        \"\"\"\n",
    "        Initialise un nouveau noeud avec une valeur spécifique et optionnellement des parents.\n",
    "        \n",
    "        Args:\n",
    "            valeur (np.ndarray): La valeur du noeud.\n",
    "            parents (list, optional): Les noeuds parents du noeud actuel. Defaults to [].\n",
    "        \"\"\"\n",
    "        self.valeur = valeur.astype('float64')  # Assure que la valeur est en float64 pour la précision des calculs.\n",
    "        self.parents = parents  # Liste des noeuds dont dépend ce noeud.\n",
    "\n",
    "        # Initialisation du gradient à un tableau de zéros de même forme que la valeur.\n",
    "        self.grad = np.zeros_like(self.valeur)  # dA\n",
    "        self._backward = lambda: None  # Fonction lambda vide pour le backward pass. Sera définie par des opérations spécifiques.\n",
    "\n",
    "    def __add__(self, B):\n",
    "        \"\"\"\n",
    "        Effectue l'addition de deux noeuds en prenant en compte le broadcasting.\n",
    "\n",
    "        Args:\n",
    "            B (Noeud): Le deuxième opérande de l'addition.\n",
    "\n",
    "        Returns:\n",
    "            Noeud: Un nouveau noeud résultant de l'addition de `self` et `B`.\n",
    "\n",
    "        Note:\n",
    "            Cette méthode ajuste automatiquement les dimensions des opérandes\n",
    "            pour le broadcasting conformément aux règles de numpy.\n",
    "        \"\"\"\n",
    "        # Initialise les variables pour les opérandes, permettant le broadcasting si nécessaire.\n",
    "        A_2, B_2 = self, B \n",
    "\n",
    "        # Gère le broadcasting sur les lignes si les dimensions ne correspondent pas.\n",
    "        if self.valeur.shape[0] != B.valeur.shape[0]:\n",
    "            if B.valeur.shape[0] == 1:  # Cas où B est broadcasté.\n",
    "                p = self.valeur.shape[0]\n",
    "                B_2 = Noeud(np.ones((p, 1))) * B\n",
    "            elif self.valeur.shape[0] == 1:  # Cas où self est broadcasté.\n",
    "                p = B.valeur.shape[0]\n",
    "                A_2 = Noeud(np.ones((p, 1))) * self\n",
    "            else:  # Erreur si le broadcasting n'est pas possible.\n",
    "                print('Erreur de dimension pour le broadcasting des lignes.')\n",
    "\n",
    "        # Gère le broadcasting sur les colonnes si nécessaire.\n",
    "        elif self.valeur.shape[1] != B.valeur.shape[1]:\n",
    "            if B.valeur.shape[1] == 1:  # Cas où B est broadcasté.\n",
    "                p = self.valeur.shape[1]\n",
    "                B_2 = B * Noeud(np.ones((1, p))) \n",
    "            elif self.valeur.shape[1] == 1:  # Cas où self est broadcasté.\n",
    "                p = B.valeur.shape[1]\n",
    "                A_2 = self * Noeud(np.ones((1, p)))\n",
    "            else:  # Erreur si le broadcasting n'est pas possible.\n",
    "                print('Erreur de dimension pour le broadcasting des colonnes.')\n",
    "\n",
    "        # Crée un nouveau noeud résultant de l'addition.\n",
    "        C = Noeud(A_2.valeur + B_2.valeur, parents=[A_2, B_2])\n",
    "\n",
    "        # Définit la fonction de backward pour le noeud résultant.\n",
    "        def _backward():\n",
    "            # Ajoute le gradient du noeud résultant aux gradients des parents.\n",
    "            A_2.grad += C.grad\n",
    "            B_2.grad += C.grad\n",
    "        C._backward = _backward\n",
    "\n",
    "        return C\n",
    "\n",
    "    def __mul__(self, B):\n",
    "        \"\"\"\n",
    "        Effectue la multiplication matricielle de deux noeuds.\n",
    "\n",
    "        Args:\n",
    "            B (Noeud): Le deuxième opérande de la multiplication.\n",
    "\n",
    "        Returns:\n",
    "            Noeud: Un nouveau noeud résultant de la multiplication matricielle de `self` et `B`.\n",
    "        \"\"\"\n",
    "        # Crée un nouveau noeud résultant de la multiplication matricielle.\n",
    "        C = Noeud(np.matmul(self.valeur, B.valeur), parents=[self, B])\n",
    "\n",
    "        # Définit la fonction de backward pour ce noeud.\n",
    "        def _backward():\n",
    "            # Calcule et ajoute les gradients par rapport à chaque parent.\n",
    "            self.grad += np.matmul(C.grad, B.valeur.T)\n",
    "            B.grad += np.matmul(self.valeur.T, C.grad)\n",
    "        C._backward = _backward\n",
    "\n",
    "        return C\n",
    "\n",
    "    \n",
    "    def somme_lignes(self):\n",
    "        \"\"\"\n",
    "        Calcule la somme des éléments de chaque ligne du noeud.\n",
    "\n",
    "        Returns:\n",
    "            Noeud: Un nouveau noeud résultant de la somme des lignes du noeud actuel.\n",
    "        \"\"\"\n",
    "        # Multiplie le noeud actuel par un vecteur colonne de uns pour obtenir la somme des lignes.\n",
    "        C = self * Noeud(np.ones((self.valeur.shape[1], 1)))\n",
    "        return C\n",
    "\n",
    "    def somme_colonnes(self):\n",
    "        \"\"\"\n",
    "        Calcule la somme des éléments de chaque colonne du noeud.\n",
    "\n",
    "        Returns:\n",
    "            Noeud: Un nouveau noeud résultant de la somme des colonnes du noeud actuel.\n",
    "        \"\"\"\n",
    "        # Multiplie un vecteur ligne de uns par le noeud actuel pour obtenir la somme des colonnes.\n",
    "        C = Noeud(np.ones((1, self.valeur.shape[0]))) * self\n",
    "        return C\n",
    "\n",
    "    def somme(self):\n",
    "        \"\"\"\n",
    "        Calcule la somme de tous les éléments du noeud.\n",
    "\n",
    "        Returns:\n",
    "            Noeud: Un nouveau noeud représentant la somme totale des éléments du noeud actuel.\n",
    "        \"\"\"\n",
    "        # Effectue la somme des lignes puis la somme des colonnes du résultat pour obtenir la somme totale.\n",
    "        lignes = self.somme_lignes()\n",
    "        colonnes = lignes.somme_colonnes()\n",
    "        somme_finale = colonnes\n",
    "        return somme_finale\n",
    "\n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Applique l'exponentielle à chaque élément du noeud.\n",
    "\n",
    "        Returns:\n",
    "            Noeud: Un nouveau noeud résultant de l'application de l'exponentielle aux éléments du noeud actuel.\n",
    "        \"\"\"\n",
    "        # Crée un nouveau noeud avec l'exponentielle de la valeur du noeud actuel.\n",
    "        C = Noeud(np.exp(self.valeur), parents=[self])\n",
    "\n",
    "        # Définit la fonction de backward pour ce noeud.\n",
    "        def _backward():\n",
    "            # Calcule le gradient par rapport au noeud actuel en utilisant la dérivée de l'exponentielle.\n",
    "            self.grad += np.multiply(C.valeur, C.grad)\n",
    "        C._backward = _backward\n",
    "\n",
    "        return C\n",
    "\n",
    "    \n",
    "    def clear_grads(self):\n",
    "        \"\"\"\n",
    "        Réinitialise les gradients à 0 (en prenant en compte que nos gradients ne sont plus des floats)\n",
    "\n",
    "        Returns: \n",
    "            Rien du tout\n",
    "        \"\"\"\n",
    "        a_visiter = self.parents.copy()\n",
    "        \n",
    "        while not len(a_visiter) == 0:\n",
    "            s = a_visiter.pop()\n",
    "            s.grad = np.zeros_like(s.valeur) \n",
    "        \n",
    "        for parent in s.parents:\n",
    "            if parent not in a_visiter:\n",
    "                a_visiter.append(parent)\n",
    "\n",
    "    def backward(self):\n",
    "        # Remettre à 0 les gradients\n",
    "        self.clear_grads()\n",
    "\n",
    "        dejaVu = set()\n",
    "        L = []\n",
    "\n",
    "        def parcours(s):\n",
    "            dejaVu.add(s)\n",
    "            \n",
    "            for parent in s.parents:\n",
    "                if parent not in dejaVu:\n",
    "                    parcours(parent)\n",
    "\n",
    "                # fin traitement de s\n",
    "            L.append(s)\n",
    "                \n",
    "        parcours(self)\n",
    "\n",
    "        self.grad = np.array([[1]]) # Il s'agit du gradient de la sortie par rapport à elle même, compatible avec les opérations numpy\n",
    "        for s in reversed(L):\n",
    "            s._backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un exemple de graphe de calcul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[40.]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = Noeud(np.array([[2,3],[1,0]]))\n",
    "B = Noeud(np.array([[1,2], [3,4]]))\n",
    "D = Noeud(np.array([[3,3], [2,2]])) # On implémentera ensuite le broadcasting \n",
    "d = Noeud(np.array([[3], [2]]))\n",
    "\n",
    "C =  A * B\n",
    "E = C + d\n",
    "J = E.somme()\n",
    "\n",
    "J.valeur # On a le même résultat selon que l'on utilise d ou D! Ceci grâce au broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "J.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rapide rappel sur le parcours en profondeur d'un graphe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noeud:\n",
    "    def __init__(self, nom, parents=None):\n",
    "        self.nom = nom\n",
    "        if parents is None:\n",
    "            self.parents = []\n",
    "        else:\n",
    "            self.parents = parents\n",
    "\n",
    "def parcours_profondeur(s, dejaVu=None, L=None): # dejaVu et L en argument pour éviter d'être réinitialisés à chaque appel récursif\n",
    "    if dejaVu is None:\n",
    "        dejaVu = set()\n",
    "    if L is None:\n",
    "        L = []\n",
    "\n",
    "    dejaVu.add(s)\n",
    "    for parent in s.parents:\n",
    "        if parent not in dejaVu:\n",
    "            parcours_profondeur(parent, dejaVu, L)\n",
    "\n",
    "    L.append(s)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '1', '2', '4', '5', '6']"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création d'un graphe similaire à celui utilisé dans notre exemple précédent \n",
    "noeud1 = Noeud(\"1\")\n",
    "noeud2 = Noeud(\"2\")\n",
    "noeud3 = Noeud(\"3\")\n",
    "noeud4 = Noeud(\"4\", [noeud1, noeud2])\n",
    "noeud5 = Noeud(\"5\", [noeud3, noeud4])\n",
    "noeud6 = Noeud(\"6\", [noeud5])\n",
    "\n",
    "# Parcours en profondeur du graphe à partir du noeud 4\n",
    "L = parcours_profondeur(noeud6)\n",
    "noms_parcours = [noeud.nom for noeud in L]\n",
    "\n",
    "noms_parcours\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
